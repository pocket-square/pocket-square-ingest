
Computer multitasking - Wikipedia
 
Computer multitasking
From Wikipedia, the free encyclopedia
					Jump to:					navigation, 					search
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2012) (Learn how and when to remove this template message)
Modern desktop operating systems are capable of handling large numbers of different processes at the same time. This screenshot shows Linux Mint running simultaneously Xfce desktop environment, Firefox, a calculator program, the built-in calendar, Vim, GIMP, and VLC media player.
Multitasking capabilities of Microsoft Windows 1.01 released in 1985, here shown running the MS-DOS Executive and Calculator programs
In computing, multitasking is a concept of performing multiple tasks (also known as processes) over a certain period of time by executing them concurrently. New tasks start and interrupt already started ones before they have reached completion, instead of executing the tasks sequentially so each started task needs to reach its end before a new one is started. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory.
Multitasking does not necessarily mean that multiple tasks are executing at exactly the same time (simultaneously). In other words, multitasking does not imply parallel execution, but it does mean that more than one task can be part-way through execution at the same time, and that more than one task is advancing over a given period of time.[1] Even on multiprocessor or multicore computers, which have multiple CPUs/cores so more than one task can be executed at once (physically, one per CPU or core), multitasking allows many more tasks to be run than there are CPUs.
In the case of a computer with a single CPU, only one task is said to be running at any point in time, meaning that the CPU is actively executing instructions for that task. Multitasking solves the problem by scheduling which task may be the one running at any given time, and when another waiting task gets a turn. The act of reassigning a CPU from one task to another one is called a context switch; the illusion of parallelism is achieved when context switches occur frequently enough. Operating systems may adopt one of many different scheduling strategies, which generally fall into the following categories:[citation needed]
In multiprogramming systems, the running task keeps running until it performs an operation that requires waiting for an external event (e.g. reading from a tape) or until the computer's scheduler forcibly swaps the running task out of the CPU. Multiprogramming systems are designed to maximize CPU usage.
In time-sharing systems, the running task is required to relinquish the CPU, either voluntarily or by an external event such as a hardware interrupt. Time sharing systems are designed to allow several programs to execute apparently simultaneously.
In real-time systems, some waiting tasks are guaranteed to be given the CPU when an external event occurs. Real time systems are designed to control mechanical devices such as industrial robots, which require timely processing.
The term "multitasking" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Danish and Norwegian.
Contents
1 Multiprogramming
2 Cooperative multitasking
3 Preemptive multitasking
4 Real time
5 Multithreading
6 Memory protection
7 Memory swapping
8 Programming
9 See also
10 References
Multiprogramming[edit]
In the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was usually very inefficient.
The first computer using a multiprogramming system was the British Leo III owned by J. Lyons and Co. During batch processing, several different programs were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.[citation needed]
The use of multiprogramming was enhanced by the arrival of virtual memory and virtual machine technology, which enabled individual programs to make use of memory and operating system resources as if other concurrently running programs were, for all practical purposes, non-existent and invisible to them.[citation needed]
Multiprogramming doesn't give any guarantee that a program will run in a timely manner. Indeed, the very first program may very well run for hours without needing access to a peripheral. As there were no users waiting at an interactive terminal, this was no problem: users handed in a deck of punched cards to an operator, and came back a few hours later for printed results. Multiprogramming greatly reduced wait times when multiple batches were being processed.[citation needed]
Cooperative multitasking[edit]
Main article: Non-preemptive multitasking
The expression "time sharing" usually designated computers shared by interactive users at terminals, such as IBM's TSO, and VM/CMS. The term "time-sharing" is no longer commonly used, having been replaced by "multitasking", following the advent of personal computers and workstations rather than shared interactive systems.
Early multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the scheduling scheme employed by Microsoft Windows (prior to Windows 95 and Windows NT) and Classic Mac OS (prior to Mac OS X) in order to enable multiple applications to be run simultaneously. Windows 9x also used cooperative multitasking, but only for 16-bit legacy applications, much the same way as pre-Leopard PowerPC versions of Mac OS X used it for Classic applications. The network operating system NetWare used cooperative multitasking up to NetWare 6.5. Cooperative multitasking is still used today on RISC OS systems.[2]
As a cooperatively multitasked system relies on each process regularly giving up time to other processes on the system, one poorly designed program can consume all of the CPU time for itself, either by performing extensive calculations or by busy waiting; both would cause the whole system to hang. In a server environment, this is a hazard that makes the entire environment unacceptably fragile.
Preemptive multitasking[edit]
Main article: Preemption (computing)
Preemptive multitasking allows the computer system to more reliably guarantee to each process a regular "slice" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process. Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was supported on DEC's PDP-8 computers, and implemented in OS/360 MFT in 1967, in MULTICS (1964), and Unix (1969); it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives.[3]
At any specific time, processes can be grouped into two categories: those that are waiting for input or output (called "I/O bound"), and those that are fully utilizing the CPU ("CPU bound"). In primitive systems, the software would often "poll", or "busywait" while waiting for requested input (such as disk, keyboard or network input). During this time, the system was not performing useful work. With the advent of interrupts and preemptive multitasking, I/O bound processes could be "blocked", or put on hold, pending the arrival of the necessary data, allowing other processes to utilize the CPU. As the arrival of the requested data would generate an interrupt, blocked processes could be guaranteed a timely return to execution.[citation needed]
The earliest preemptive multitasking OS available to home users was Sinclair QDOS on the Sinclair QL, released in 1984, but very few people bought the machine. Commodore's powerful Amiga, released the following year, was the first commercially successful home computer to use the technology, and its multimedia abilities make it a clear ancestor of contemporary multitasking personal computers. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. It was later adopted on the Apple Macintosh by Mac OS X that, as a Unix-like operating system, uses preemptive multitasking for all native applications.
A similar model is used in Windows 9x and the Windows NT family, where native 32-bit applications are multitasked preemptively, and legacy 16-bit Windows 3.x programs are multitasked cooperatively within a single process, although in the NT family it is possible to force a 16-bit application to run as a separate preemptively multitasked process.[4] 64-bit editions of Windows, both for the x86-64 and Itanium architectures, no longer provide support for legacy 16-bit applications, and thus provide preemptive multitasking for all supported applications.
Real time[edit]
Another reason for multitasking was in the design of real-time computing systems, where there are a number of possibly unrelated external activities needed to be controlled by a single processor system. In such systems a hierarchical interrupt system is coupled with process prioritization to ensure that key activities were given a greater share of available process time.[citation needed]
Multithreading[edit]
As multitasking greatly improved the throughput of computers, programmers started to implement applications as sets of cooperating processes (e. g., one process gathering input data, one process processing input data, one process writing out results on disk). This, however, required some tools to allow processes to efficiently exchange data.[citation needed]
Threads were born from the idea that the most efficient way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open files. Threads are described as lightweight processes because switching between threads does not involve changing the memory context.[5][6][7]
While threads are scheduled preemptively, some operating systems provide a variant to threads, named fibers, that are scheduled cooperatively. On operating systems that do not provide fibers, an application may implement its own fibers using repeated calls to worker functions. Fibers are even more lightweight than threads, and somewhat easier to program with, although they tend to lose some or all of the benefits of threads on machines with multiple processors.[citation needed]
Some systems directly support multithreading in hardware.
Memory protection[edit]
Main article: Memory protection
Essential to any multitasking system is to safely and effectively share access to system resources. Access to memory must be strictly managed to ensure that no process can inadvertently or deliberately read or write to memory locations outside of the process's address space. This is done for the purpose of general system stability and data integrity, as well as data security.
In general, memory access management is a responsibility of the operating system kernel, in combination with hardware mechanisms that provide supporting functionalities, such as a memory management unit (MMU). If a process attempts to access a memory location outside of its memory space, the MMU denies the request and signals the kernel to take appropriate actions; this usually results in forcibly terminating the offending process. Depending on the software and kernel design and the specific error in question, the user may receive an access violation error message such as "segmentation fault".
In a well designed and correctly implemented multitasking system, a given process can never directly access memory that belongs to another process. An exception to this rule is in the case of shared memory; for example, in the System V inter-process communication mechanism the kernel allocates memory to be mutually shared by multiple processes. Such features are often used by database management software such as PostgreSQL.
Inadequate memory protection mechanisms, either due to flaws in their design or poor implementations, allow for security vulnerabilities that may be potentially exploited by malicious software.
Memory swapping[edit]
Use of a swap file or swap partition is a way for the operating system to provide more memory than is physically available by keeping portions of the primary memory in secondary storage. While multitasking and memory swapping are two completely unrelated techniques, they are very often used together, as swapping memory allows more tasks to be loaded at the same time. Typically, a multitasking system allows another process to run when the running process hits a point where it has to wait for some portion of memory to be reloaded from secondary storage.[citation needed]
Programming[edit]
Processes that are entirely independent are not much trouble to program in a multitasking environment. Most of the complexity in multitasking systems comes from the need to share computer resources between tasks and to synchronize the operation of co-operating tasks.[citation needed]
Various concurrent computing techniques are used to avoid potential problems caused by multiple tasks attempting to access the same resource.[citation needed]
Bigger systems were sometimes built with a central processor(s) and some number of I/O processors, a kind of asymmetric multiprocessing.[citation needed]
Over the years, multitasking systems have been refined. Modern operating systems generally include detailed mechanisms for prioritizing processes, while symmetric multiprocessing has introduced new complexities and capabilities.[citation needed] In other words, if you were in need of a smaller explanation, it is the task of doing two (or more) things at one time.
See also[edit]
Process state
Task switching
References[edit]
^ "Concurrency vs Parallelism, Concurrent Programming vs Parallel Programming". Oracle. Retrieved March 23, 2016. 
^ "Preemptive multitasking". riscos.info. 2009-11-03. Retrieved 2014-07-27. 
^ "UNIX, Part One". The Digital Research Initiative. ibiblio.org. 2002-01-30. Retrieved 2014-01-09. 
^ Smart Computing Article - Windows 2000 &16-Bit Applications[dead link]
^ Eduardo Ciliendo; Takechika Kunimasa (April 25, 2008). "Linux Performance and Tuning Guidelines" (PDF). redbooks.ibm.com. IBM. p. 4. Retrieved March 1, 2015. 
^ "Context Switch Definition". linfo.org. May 28, 2006. Retrieved February 26, 2015. 
^ "What are threads (user/kernel)?". tldp.org. September 8, 1997. Retrieved February 26, 2015. 
v
t
e
Operating systems
General
Advocacy
Comparison
Forensic engineering
History
Hobbyist development
List
Timeline
Usage share
Kernel
Architectures
Exokernel
Hybrid
Microkernel
Monolithic
Rump kernel
Unikernel
Components
Device driver
Loadable kernel module
Microkernel
User space
Process management
Concepts
Context switch
Interrupt
IPC
Process
Process control block
Thread
Time-sharing
Scheduling
algorithms
Computer multitasking
Fixed-priority preemptive
Multilevel feedback queue
Preemptive
Round-robin
Shortest job next
Memory management and
resource protection
Bus error
General protection fault
Memory protection
Paging
Protection ring
Segmentation fault
Virtual memory
Storage access and
file systems
Boot loader
Defragmentation
Device file
File attribute
Inode
Journal
Partition
Virtual file system
Virtual tape library
List
AmigaOS
Android
BeOS
BSD
Chrome OS
CP/M
DOS
Hurd
Haiku
illumos
iOS
Linux
Macintosh
Classic Mac OS
macOS
MINIX
MorphOS
MUSIC/SP
NOS
OpenVMS
ORVYL
OS/2
OSv
Pick
QNX
ReactOS
RISC OS
RSTS/E
RSX-11
RT-11
Solaris
TOPS-10/TOPS-20
TPF
Unix
Visi On
VM/CMS
VS/9
Windows
Xinu
z/OS
Miscellaneous concepts
API
Computer network
HAL
Live CD
Live USB
OS shell
CLI
GUI
TUI
VUI
PXE
v
t
e
Parallel computing
General
Massively parallel (computing)
Distributed computing
Parallel computing
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Speculative (SpMT)
Preemptive
Cooperative
Clustered Multi-Thread (CMT)
Hardware scout
Theory
PRAM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
Karp–Flatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Coordination
Multiprocessing
Memory coherency
Cache coherency
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream Processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
SIMT
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Grid computer
APIs
Ateji PX
Boost.Thread
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
TPL
PLINQ
PVM
POSIX Threads
RaftLib
UPC
TBB
Problems
Embarrassingly parallel
Software lockout
Scalability
Race condition
Deadlock
Livelock
Starvation
Deterministic algorithm
Parallel slowdown
 Category: parallel computing
 Media related to Parallel computing at Wikimedia Commons
 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Computer_multitasking&oldid=758601072"					
Categories: Concurrent computingOperating system technologyHidden categories: All articles with dead external linksArticles with dead external links from June 2016Articles needing additional references from February 2012All articles needing additional referencesAll articles with unsourced statementsArticles with unsourced statements from July 2013Articles with unsourced statements from August 2007 
Navigation menu
Personal tools
Not logged inTalkContributionsCreate accountLog in 
Namespaces
Article
Talk
Variants
Views
Read
Edit
View history
More
Search
 
Navigation
Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 
Interaction
HelpAbout WikipediaCommunity portalRecent changesContact page 
Tools
What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 
Print/export
Create a bookDownload as PDFPrintable version 
Languages
CatalàČeštinaDanskDeutschEestiEspañolEsperantoEuskaraفارسیFrançais한국어ՀայերենHrvatskiBahasa IndonesiaÍslenskaItalianoעבריתҚазақшаMagyarNederlands日本語Norsk bokmålNorsk nynorskPolskiPortuguêsРусскийSimple EnglishSlovenčinaSlovenščinaکوردیی ناوەندیSuomiSvenskaУкраїнська中文 
Edit links 
 This page was last modified on 6 January 2017, at 12:34.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view
 
 
